# SSIM Loss v2: 梯度缩放 + Timestep 收紧修复方案

> 日期: 2026-02-06
> 前序版本: v1 (2026-02-05, 见 `20260205_SSIM_Loss_集成技术文档.md`)
> 状态: 已实现, 待训练验证

---

## 1. 问题: v1 推理坍塌 (Inference Collapse)

### 1.1 现象

v1 版本 SSIM loss 训练时:
- **Loss 曲线正常**: SSIM loss 在 `[0, 1]` 范围内, 看起来在收敛
- **推理结果坍塌**: 训练 ~2000 步后, 生成图像完全崩溃 (纯噪声/纯色块)
- **对比**: 相同架构下 Pixel loss (L2) 训练正常, 不会坍塌

### 1.2 根因分析: SSIM 梯度爆炸

**核心问题**: SSIM 梯度通过 VAE 解码器反传时, 比 L2 像素损失的梯度大 **200-2000 倍**.

| 因素 | Pixel Loss (L2, 正常) | SSIM Loss (v1, 坍塌) |
|------|----------------------|---------------------|
| Loss 权重 | 0.05 | 1.0 (20x 更大) |
| 每像素梯度量级 | O(1), 简单差值 | O(10-100), 有理函数 |
| **有效梯度** | **O(0.05)** | **O(10-100)** |

**为什么 SSIM 梯度这么大?**

SSIM 的公式包含除法运算:

```
SSIM = numerator / (denominator + 1e-8)

其中 denominator = (sigma_x_sq + sigma_y_sq + C2)
     C2 = (0.03 * 1.0)^2 = 0.0009
```

在**均匀区域或低对比度区域** (医学图像中很常见):
- `sigma_x_sq ≈ 0`, `sigma_y_sq ≈ 0`
- `denominator ≈ C2 = 0.0009` (极小值)
- 梯度 ∝ `1 / denominator^2` → 产生梯度尖峰

对比 L2 loss:
```
L2 = (x_pred - x_gt)^2
∂L2/∂x_pred = 2 * (x_pred - x_gt)   ← 没有除法, 梯度平滑
```

**结果**: 过大的 SSIM 梯度在反传时腐蚀 LLM 和 diffusion 权重. Loss 本身在 `[0,1]` 范围内看起来正常 (因为 SSIM 值有界), 但梯度量级不受约束, 导致模型的生成能力被悄悄摧毁.

### 1.3 为什么 Loss 曲线看不出问题

```
SSIM ∈ [-1, 1]  →  Loss = 1 - SSIM ∈ [0, 2]  →  clamp 后 ∈ [0, 1]
```

Loss 的**值**有界, 但 Loss 对 VAE 解码输出的**梯度**无界. 这是一个典型的 "loss 看着正常但梯度有毒" 的陷阱.

---

## 2. 修复方案: 三层梯度衰减

### 2.1 总览

| 层级 | 参数 | v1 值 | v2 值 | 效果 |
|------|------|-------|-------|------|
| 1. Loss 权重 | `ssim_loss_weight` | 1.0 | **0.1** | 10x 缩小 |
| 2. Timestep 门控 | `ssim_loss_max_t` | 0.3 | **0.1** | 更少样本参与, 只取干净图 |
| 3. 梯度缩放 | `ssim_grad_scale` | N/A (1.0) | **0.1** | 梯度直接 10x 衰减 |

**净效果**: `0.1 (weight) × 0.1 (grad_scale) × O(10-100) = O(0.1-1.0)`

与 Pixel loss 的 O(0.05) 量级一致, 不再腐蚀权重.

### 2.2 第一层: Loss 权重 (ssim_loss_weight: 1.0 → 0.1)

最直观的调节. 总 loss 中 SSIM 的贡献缩小 10 倍:

```python
loss += ssim * ssim_loss_weight  # 0.1 * ssim, 而不是 1.0 * ssim
```

单独调这一项不够, 因为梯度量级 O(10-100) 乘以 0.1 仍然是 O(1-10), 还是比 pixel loss 大.

### 2.3 第二层: Timestep 收紧 (ssim_loss_max_t: 0.3 → 0.1)

**原理**: 在 diffusion 框架中, noisy latent = `(1-t) * z0 + t * noise`:

| t 值 | 信号占比 | 噪声占比 | SSIM 可靠性 |
|------|---------|---------|------------|
| 0.1 | 90% | 10% | 高: 局部统计量可靠 |
| 0.3 (v1) | 70% | 30% | 低: 噪声干扰局部方差/协方差 |
| 0.5 | 50% | 50% | 很低: SSIM 基本在比较噪声 |

- **Pixel L2 loss 能容忍 t=0.3**: 因为它是简单的逐像素差, 噪声只是增加了均匀的误差
- **SSIM 不能容忍 t=0.3**: 卷积窗口内的局部统计量 (方差/协方差) 被噪声严重扭曲, 梯度方向不可靠

收紧到 `max_t=0.1` 后:
- 参与 SSIM 的样本更少 (只有 t ≤ 0.1 的)
- 每个参与样本的梯度方向更可靠 (信号占 90%+)

### 2.4 第三层: 梯度缩放 (ssim_grad_scale: 新增, 默认 0.1)

这是最关键的新机制. 在 VAE 解码器输出上应用 **straight-through gradient scaling**:

```python
def _scale_gradient(x: torch.Tensor, scale: float) -> torch.Tensor:
    """Forward: 返回 x 不变. Backward: 梯度乘以 scale."""
    return x.detach() + scale * (x - x.detach())
```

**数学等价性**:
- Forward: `_scale_gradient(x, 0.1)` = `x` (值完全不变)
- Backward: `d(loss)/d(x)` → `0.1 * d(loss)/d(x)` (梯度缩小 10 倍)

**应用位置**: VAE 解码之后, SSIM 计算之前:

```python
# losses.py 中 compute_perceptual_loss() 的 chunked decode 循环
x_pred_chunk = vae_decode_fn(latent_chunk)           # VAE 解码
x_pred_chunk = x_pred_chunk[:, :, :max_h_img, :max_w_img]  # 裁 padding

# NEW: 梯度缩放, 防止 SSIM 梯度爆炸
if ssim_grad_scale < 1.0:
    x_pred_chunk = _scale_gradient(x_pred_chunk, ssim_grad_scale)

# 后续 SSIM 计算不受影响 (forward 值不变)
x_pred_chunk_01 = (x_pred_chunk * 0.5 + 0.5).clamp(0, 1)
```

**为什么放在 VAE 解码之后**:

```
LLM → llm2vae → z0_pred → VAE decode → x_pred → SSIM → loss
                                        ↑
                           梯度缩放放在这里, 截断 SSIM 的大梯度
                           向 VAE/LLM 传播
```

梯度缩放只影响 SSIM 梯度通过 VAE 解码器向 LLM 和 diffusion 模块反传的路径. 不影响:
- MSE loss 的梯度 (在 latent space, 不经过 VAE decode)
- CE loss 的梯度 (语言模型, 完全独立)
- SSIM loss 的 forward 值 (用于日志和 TensorBoard, 完全不变)

---

## 3. 修改的文件 (4 个)

### 3.1 `modeling/bagel/losses.py`

**新增 `_scale_gradient()` 函数** (line ~1046):

```python
def _scale_gradient(x: torch.Tensor, scale: float) -> torch.Tensor:
    """Scale backward gradient by `scale` without changing forward value."""
    return x.detach() + scale * (x - x.detach())
```

**`compute_perceptual_loss()` 签名新增参数**:

```python
ssim_grad_scale: float = 0.1,   # 新增
```

**VAE decode 后应用梯度缩放**:

```python
x_pred_chunk = vae_decode_fn(latent_chunk)
x_pred_chunk = x_pred_chunk[:, :, :max_h_img, :max_w_img]
# NEW:
if ssim_grad_scale < 1.0:
    x_pred_chunk = _scale_gradient(x_pred_chunk, ssim_grad_scale)
```

### 3.2 `modeling/bagel/bagel.py`

**`forward()` 签名变更**:

```python
# v1:
ssim_loss_max_t: float = 0.3,
# v2:
ssim_loss_max_t: float = 0.1,     # 收紧
ssim_grad_scale: float = 0.1,     # 新增
```

**`compute_perceptual_loss()` 调用新增参数**:

```python
ssim = compute_perceptual_loss(
    ...,
    ssim_grad_scale=ssim_grad_scale,   # 新增
    ...
)
```

### 3.3 `train/main_sr_pixel_loss.py`

**`TrainingArguments` 变更**:

```python
# v1:
ssim_loss_max_t: float = 0.3
# v2:
ssim_loss_max_t: float = 0.1      # 收紧

# 新增:
ssim_grad_scale: float = 0.1      # 梯度缩放
```

**data dict 新增**:

```python
data['ssim_grad_scale'] = training_args.ssim_grad_scale
```

### 3.4 `scripts/training/train_sft_stage1_medq_unif_multinode_eyeQ1_sr_ssim_loss.sh`

**变量变更**:

```bash
# v1:
SSIM_LOSS_WEIGHT=1.0
SSIM_LOSS_MAX_T=0.3

# v2:
SSIM_LOSS_WEIGHT=0.1       # 10x 缩小
SSIM_LOSS_MAX_T=0.1        # 收紧
SSIM_GRAD_SCALE=0.1        # 新增
```

**torchrun 参数新增**:

```bash
--ssim_grad_scale "${SSIM_GRAD_SCALE}"
```

**实验名更新** (避免 auto_resume 到旧的损坏 checkpoint):

```bash
# v1:
EXP_NAME="${1:-stage1_medq_2nodes_unif_eyeQ1_sr_ssim_loss}"
# v2:
EXP_NAME="${1:-stage1_medq_2nodes_unif_eyeQ1_sr_ssim_loss_v2}"
```

---

## 4. 数据流图: 梯度缩放的作用位置

```
                        Forward (值不变)
                        ─────────────→

LLM hidden  ──→  llm2vae  ──→  z0_pred tokens
    ↑                              │
    │                              │ unpatchify
    │                              ▼
    │                         z0_pred latent (C, H, W)
    │                              │
    │                              │ vae_decode_fn()
    │                              ▼
    │                         x_pred (pixel space)
    │                              │
    │         ┌────────────────────┤
    │         │ _scale_gradient()  │
    │         │ forward: x_pred    │  ← 值不变, SSIM 计算正确
    │         │ backward: grad×0.1 │  ← 梯度缩小 10 倍
    │         └────────────────────┤
    │                              │
    │                              ▼
    │                    normalize to [0,1]
    │                              │
    │                              ▼
    │                    compute_ssim_loss()
    │                              │
    │                              ▼
    │                      ssim_loss scalar
    │                              │
    │                              │ × ssim_loss_weight (0.1)
    │                              ▼
    │                          total loss
    │                              │
    │         ←─────────────────────
    │              Backward (梯度缩小后反传)
    │
    │    此时到达 LLM 的梯度:
    │    = ssim_loss_weight × ssim_grad_scale × ∂SSIM/∂x_pred × ∂VAE/∂z0 × ∂z0/∂LLM
    │    = 0.1 × 0.1 × O(10-100) × ...
    │    = O(0.1-1.0) × ...
    │    ≈ pixel loss 的量级  ✓
```

---

## 5. 参数调优指南

### 5.1 保守 → 激进调参路线

如果 v2 默认参数 (三个都是 0.1) 训练稳定但 SSIM 效果不明显, 可以逐步放松:

| 阶段 | ssim_loss_weight | ssim_loss_max_t | ssim_grad_scale | 预期有效梯度 |
|------|------------------|-----------------|-----------------|-------------|
| 保守 (v2 默认) | 0.1 | 0.1 | 0.1 | O(0.1-1.0) |
| 中等 | 0.2 | 0.1 | 0.1 | O(0.2-2.0) |
| 较激进 | 0.3 | 0.15 | 0.1 | O(0.3-3.0) |
| 激进 (接近 pixel 原始) | 0.5 | 0.2 | 0.1 | O(0.5-5.0) |

**规则**:
- `ssim_grad_scale` 建议保持 0.1 不动 (这是防梯度爆炸的核心安全阀)
- 先调 `ssim_loss_weight` (最直观)
- 再调 `ssim_loss_max_t` (扩大参与样本范围)
- 观察 `total_norm` 是否飙升, 如果飙升说明梯度太大

### 5.2 Debug 模式验证

训练前跑 10-50 步开启 debug:

```bash
SSIM_LOSS_DEBUG=1 bash scripts/training/train_sft_stage1_medq_unif_multinode_eyeQ1_sr_ssim_loss.sh
```

**检查项**:

| 检查项 | 期望值 | 异常信号 |
|--------|--------|---------|
| 日志中 `ssim_grad_scale=0.1` | 出现 | 没出现 = 参数未传递 |
| `ssim_loss` 值 | [0, 1], 通常 0.3-0.8 | NaN / Inf / >1.0 |
| `total_norm` | 与 pixel-loss-only 训练相当 | 比 pixel-loss 大 10x+ = 梯度仍然过大 |
| 推理结果 (~2000 步后) | 有结构的图像 | 纯噪声/纯色块 = 仍然坍塌 |

### 5.3 与 Pixel Loss 联合使用

v2 也支持 pixel + SSIM 混合模式:

```bash
PIXEL_LOSS_WEIGHT=0.05    # L2 pixel loss
SSIM_LOSS_WEIGHT=0.1      # SSIM loss
SSIM_LOSS_MAX_T=0.1
SSIM_GRAD_SCALE=0.1
```

此时总的像素空间 loss = `0.05 * L2 + 0.1 * SSIM`. 两者各自独立执行 VAE decode (不共享).

---

## 6. v1 vs v2 对比

```
                    v1 (坍塌)                    v2 (修复)
─────────────────────────────────────────────────────────────
ssim_loss_weight    1.0                          0.1
ssim_loss_max_t     0.3                          0.1
ssim_grad_scale     无 (等效 1.0)                 0.1
EXP_NAME            ..._ssim_loss                ..._ssim_loss_v2
─────────────────────────────────────────────────────────────
有效梯度量级         O(10-100)                    O(0.1-1.0)
vs Pixel Loss       200-2000x 更大               ≈ 相当
Loss 曲线            正常 (SSIM 有界)              正常
推理结果             坍塌 (权重被腐蚀)             待验证 (预期正常)
─────────────────────────────────────────────────────────────
```

---

## 7. 运行方式

```bash
# 默认配置 (v2, 推荐)
bash scripts/training/train_sft_stage1_medq_unif_multinode_eyeQ1_sr_ssim_loss.sh

# 带 debug 日志
SSIM_LOSS_DEBUG=1 bash scripts/training/train_sft_stage1_medq_unif_multinode_eyeQ1_sr_ssim_loss.sh

# 自定义实验名 + GPU 数
bash scripts/training/train_sft_stage1_medq_unif_multinode_eyeQ1_sr_ssim_loss.sh \
  "my_ssim_v2_experiment" 4 23456
```

---

## 8. 遗留事项

- [ ] 跑 v2 训练, 确认 SSIM loss 正常下降且推理不坍塌
- [ ] 对比 total_norm: v2 SSIM vs pixel-loss-only, 确认梯度量级一致
- [ ] 2000 步推理验证: 生成图像有正常结构
- [ ] 长训练 (10000+ 步) 稳定性验证
- [ ] 可选: 尝试 ssim_loss_weight=0.2 或 0.3, 观察是否能进一步提升 SSIM 指标
- [ ] 可选: pixel + SSIM 混合实验
