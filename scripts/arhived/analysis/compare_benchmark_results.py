#!/usr/bin/env python3
"""
Benchmark Results Comparison Script

This script compares the performance of Baseline (single-expert) and MoT (full architecture)
experiments by analyzing the training_metrics.csv files.

Usage:
    python scripts/compare_benchmark_results.py

Output:
    - output/baseline_vs_mot_comparison.png (visualization)
    - output/comparison_summary.txt (detailed statistics)
    - Console output (comparison table)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import sys

# ===== Configuration =====
BASELINE_CSV = "output/benchmark_baseline_vqa_1000steps/logs/training_metrics.csv"
MOT_CSV = "output/benchmark_mot_vqa_1000steps/logs/training_metrics.csv"
OUTPUT_PLOT = "output/baseline_vs_mot_comparison.png"
OUTPUT_SUMMARY = "output/comparison_summary.txt"
WARMUP_STEPS = 100  # Exclude warmup period from statistics

def check_files_exist():
    """Check if required CSV files exist."""
    missing_files = []
    if not os.path.exists(BASELINE_CSV):
        missing_files.append(BASELINE_CSV)
    if not os.path.exists(MOT_CSV):
        missing_files.append(MOT_CSV)

    if missing_files:
        print("âŒ Error: Missing CSV files:")
        for f in missing_files:
            print(f"   - {f}")
        print("\nPlease run the benchmark experiments first:")
        print("   bash scripts/benchmark_baseline_vqa.sh")
        print("   bash scripts/benchmark_mot_vqa.sh")
        sys.exit(1)

def load_data():
    """Load CSV files and basic validation."""
    print("ğŸ“‚ Loading data...")
    print(f"   Baseline: {BASELINE_CSV}")
    print(f"   MoT:      {MOT_CSV}")

    df_baseline = pd.read_csv(BASELINE_CSV)
    df_mot = pd.read_csv(MOT_CSV)

    print(f"   Baseline: {len(df_baseline)} steps loaded")
    print(f"   MoT:      {len(df_mot)} steps loaded")

    return df_baseline, df_mot

def compute_stats(df, name):
    """Compute statistics from dataframe."""
    stats = {
        'name': name,
        'total_steps': len(df),
        'avg_ce_loss': df['ce_loss'].mean(),
        'std_ce_loss': df['ce_loss'].std(),
        'avg_mse_loss': df['mse_loss'].mean(),
        'avg_sec_per_step': df['sec_per_step'].mean(),
        'std_sec_per_step': df['sec_per_step'].std(),
        'min_sec_per_step': df['sec_per_step'].min(),
        'max_sec_per_step': df['sec_per_step'].max(),
        'avg_peak_memory_gb': df['peak_memory_gb'].mean(),
        'max_peak_memory_gb': df['peak_memory_gb'].max(),
        'min_peak_memory_gb': df['peak_memory_gb'].min(),
        'final_ce_loss': df['ce_loss'].iloc[-10:].mean(),  # Last 10 steps average
    }
    return stats

def print_comparison_table(baseline_stats, mot_stats):
    """Print comparison table to console."""
    print("\n" + "="*90)
    print("Baseline vs MoT å®éªŒç»“æœå¯¹æ¯”")
    print("="*90)
    print(f"{'æŒ‡æ ‡':<35} {'Baseline':<20} {'MoT':<20} {'å·®å¼‚':<15}")
    print("-"*90)

    metrics = [
        ('æ€»æ­¥æ•°', 'total_steps', False),
        ('å¹³å‡ CE Loss', 'avg_ce_loss', False),
        ('æœ€ç»ˆ CE Loss (last 10 steps)', 'final_ce_loss', False),
        ('å¹³å‡ Sec/Step', 'avg_sec_per_step', True),
        ('Sec/Step æ ‡å‡†å·®', 'std_sec_per_step', False),
        ('æœ€å° Sec/Step', 'min_sec_per_step', False),
        ('æœ€å¤§ Sec/Step', 'max_sec_per_step', False),
        ('å¹³å‡å³°å€¼ GPU Memory (GB)', 'avg_peak_memory_gb', True),
        ('æœ€å¤§å³°å€¼ GPU Memory (GB)', 'max_peak_memory_gb', True),
    ]

    for metric_name, key, compute_overhead in metrics:
        baseline_val = baseline_stats[key]
        mot_val = mot_stats[key]

        if compute_overhead and baseline_val > 0:
            diff_pct = ((mot_val - baseline_val) / baseline_val) * 100
            print(f"{metric_name:<35} {baseline_val:<20.4f} {mot_val:<20.4f} {diff_pct:>+14.2f}%")
        else:
            diff_abs = mot_val - baseline_val
            if key in ['avg_ce_loss', 'final_ce_loss']:
                print(f"{metric_name:<35} {baseline_val:<20.6f} {mot_val:<20.6f} {diff_abs:>+14.6f}")
            else:
                print(f"{metric_name:<35} {baseline_val:<20.4f} {mot_val:<20.4f} {diff_abs:>+14.4f}")

    print("="*90)

    # Calculate and display key overheads
    time_overhead = ((mot_stats['avg_sec_per_step'] - baseline_stats['avg_sec_per_step'])
                     / baseline_stats['avg_sec_per_step']) * 100
    memory_overhead = ((mot_stats['max_peak_memory_gb'] - baseline_stats['max_peak_memory_gb'])
                       / baseline_stats['max_peak_memory_gb']) * 100

    print("\nğŸ” å…³é”®å¼€é”€æ€»ç»“ï¼š")
    print(f"   æ—¶é—´å¼€é”€ (Sec/Step):    {time_overhead:+.2f}%")
    print(f"   æ˜¾å­˜å¼€é”€ (Peak Memory): {memory_overhead:+.2f}%")
    print(f"   Loss å·®å¼‚ (Final CE):   {((mot_stats['final_ce_loss'] - baseline_stats['final_ce_loss']) / baseline_stats['final_ce_loss']) * 100:+.2f}%")
    print("="*90 + "\n")

def plot_comparison(df_baseline, df_mot, baseline_stats, mot_stats):
    """Generate comparison plots."""
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # Set Chinese font (fallback to default if not available)
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
    except:
        pass

    # 1. Sec/Step Comparison
    axes[0, 0].plot(df_baseline['step'], df_baseline['sec_per_step'],
                    label='Baseline', alpha=0.7, linewidth=1.5)
    axes[0, 0].plot(df_mot['step'], df_mot['sec_per_step'],
                    label='MoT', alpha=0.7, linewidth=1.5)
    axes[0, 0].axvline(x=WARMUP_STEPS, color='red', linestyle='--',
                       label='Warmup End', alpha=0.5)
    axes[0, 0].set_xlabel('Training Step', fontsize=12)
    axes[0, 0].set_ylabel('Sec/Step', fontsize=12)
    axes[0, 0].set_title('Training Speed Comparison', fontsize=14, fontweight='bold')
    axes[0, 0].legend(fontsize=10)
    axes[0, 0].grid(True, alpha=0.3)

    # 2. Peak Memory Comparison
    axes[0, 1].plot(df_baseline['step'], df_baseline['peak_memory_gb'],
                    label='Baseline', alpha=0.7, linewidth=1.5)
    axes[0, 1].plot(df_mot['step'], df_mot['peak_memory_gb'],
                    label='MoT', alpha=0.7, linewidth=1.5)
    axes[0, 1].axvline(x=WARMUP_STEPS, color='red', linestyle='--',
                       label='Warmup End', alpha=0.5)
    axes[0, 1].set_xlabel('Training Step', fontsize=12)
    axes[0, 1].set_ylabel('Peak GPU Memory (GB)', fontsize=12)
    axes[0, 1].set_title('GPU Memory Usage Comparison', fontsize=14, fontweight='bold')
    axes[0, 1].legend(fontsize=10)
    axes[0, 1].grid(True, alpha=0.3)

    # 3. CE Loss Comparison
    axes[1, 0].plot(df_baseline['step'], df_baseline['ce_loss'],
                    label='Baseline', alpha=0.7, linewidth=1.5)
    axes[1, 0].plot(df_mot['step'], df_mot['ce_loss'],
                    label='MoT', alpha=0.7, linewidth=1.5)
    axes[1, 0].axvline(x=WARMUP_STEPS, color='red', linestyle='--',
                       label='Warmup End', alpha=0.5)
    axes[1, 0].set_xlabel('Training Step', fontsize=12)
    axes[1, 0].set_ylabel('CE Loss', fontsize=12)
    axes[1, 0].set_title('Cross-Entropy Loss Comparison', fontsize=14, fontweight='bold')
    axes[1, 0].legend(fontsize=10)
    axes[1, 0].grid(True, alpha=0.3)

    # 4. Summary Bar Chart
    metrics_names = ['Sec/Step\n(lower better)', 'Peak Memory (GB)\n(lower better)']
    baseline_vals = [baseline_stats['avg_sec_per_step'], baseline_stats['max_peak_memory_gb']]
    mot_vals = [mot_stats['avg_sec_per_step'], mot_stats['max_peak_memory_gb']]

    x = np.arange(len(metrics_names))
    width = 0.35

    bars1 = axes[1, 1].bar(x - width/2, baseline_vals, width,
                           label='Baseline', alpha=0.8, color='#4CAF50')
    bars2 = axes[1, 1].bar(x + width/2, mot_vals, width,
                           label='MoT', alpha=0.8, color='#FF9800')

    axes[1, 1].set_ylabel('Value', fontsize=12)
    axes[1, 1].set_title('Average Performance Metrics', fontsize=14, fontweight='bold')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(metrics_names, fontsize=11)
    axes[1, 1].legend(fontsize=10)
    axes[1, 1].grid(True, alpha=0.3, axis='y')

    # Add value labels on bars
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            axes[1, 1].annotate(f'{height:.2f}',
                               xy=(bar.get_x() + bar.get_width() / 2, height),
                               xytext=(0, 3),  # 3 points vertical offset
                               textcoords="offset points",
                               ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.savefig(OUTPUT_PLOT, dpi=300, bbox_inches='tight')
    print(f"   âœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {OUTPUT_PLOT}")

def save_summary(baseline_stats, mot_stats):
    """Save detailed summary to file."""
    print("ğŸ’¾ ä¿å­˜è¯¦ç»†ç»Ÿè®¡...")

    os.makedirs(os.path.dirname(OUTPUT_SUMMARY) if os.path.dirname(OUTPUT_SUMMARY) else '.', exist_ok=True)

    with open(OUTPUT_SUMMARY, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("Baseline vs MoT å®éªŒè¯¦ç»†ç»“æœ\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n")
        f.write("="*80 + "\n\n")

        f.write(f"## Baseline ç»Ÿè®¡ (æ­¥éª¤ {WARMUP_STEPS}-{baseline_stats['total_steps']})\n")
        f.write("-"*80 + "\n")
        for key, val in baseline_stats.items():
            if isinstance(val, float):
                f.write(f"  {key}: {val:.6f}\n")
            else:
                f.write(f"  {key}: {val}\n")

        f.write("\n" + "="*80 + "\n")
        f.write(f"## MoT ç»Ÿè®¡ (æ­¥éª¤ {WARMUP_STEPS}-{mot_stats['total_steps']})\n")
        f.write("-"*80 + "\n")
        for key, val in mot_stats.items():
            if isinstance(val, float):
                f.write(f"  {key}: {val:.6f}\n")
            else:
                f.write(f"  {key}: {val}\n")

        f.write("\n" + "="*80 + "\n")
        f.write("## MoT ç›¸å¯¹ Baseline çš„å¼€é”€\n")
        f.write("-"*80 + "\n")

        time_overhead = ((mot_stats['avg_sec_per_step'] - baseline_stats['avg_sec_per_step'])
                         / baseline_stats['avg_sec_per_step']) * 100
        memory_overhead = ((mot_stats['max_peak_memory_gb'] - baseline_stats['max_peak_memory_gb'])
                           / baseline_stats['max_peak_memory_gb']) * 100
        loss_diff = ((mot_stats['final_ce_loss'] - baseline_stats['final_ce_loss'])
                     / baseline_stats['final_ce_loss']) * 100

        f.write(f"  æ—¶é—´å¼€é”€ (Sec/Step):        {time_overhead:+.2f}%\n")
        f.write(f"  æ˜¾å­˜å¼€é”€ (Peak Memory):     {memory_overhead:+.2f}%\n")
        f.write(f"  Loss å·®å¼‚ (Final CE Loss): {loss_diff:+.2f}%\n")

        f.write("\n" + "="*80 + "\n")
        f.write("## ç»“è®º\n")
        f.write("-"*80 + "\n")

        if time_overhead > 0:
            f.write(f"MoT æ¶æ„ç›¸æ¯” Baseline æ…¢ {time_overhead:.2f}%ï¼Œ")
        else:
            f.write(f"MoT æ¶æ„ç›¸æ¯” Baseline å¿« {-time_overhead:.2f}%ï¼Œ")

        if memory_overhead > 0:
            f.write(f"å¤šç”¨ {memory_overhead:.2f}% æ˜¾å­˜ã€‚\n")
        else:
            f.write(f"å°‘ç”¨ {-memory_overhead:.2f}% æ˜¾å­˜ã€‚\n")

        f.write(f"ä¸¤è€…çš„æœ€ç»ˆ Loss å·®å¼‚ä¸º {loss_diff:+.2f}%ï¼Œ")
        if abs(loss_diff) < 1.0:
            f.write("è®­ç»ƒæ•ˆæœåŸºæœ¬ä¸€è‡´ã€‚\n")
        elif loss_diff < 0:
            f.write("MoT ç•¥ä¼˜äº Baselineã€‚\n")
        else:
            f.write("Baseline ç•¥ä¼˜äº MoTã€‚\n")

        f.write("\nè¿™äº›å¼€é”€ä¸»è¦æ¥è‡ªï¼š\n")
        f.write("  1. VAE æ¨¡å‹çš„åˆå§‹åŒ–å’Œå†…å­˜å ç”¨\n")
        f.write("  2. é¢å¤–çš„æŠ•å½±å±‚ (vae2llm, llm2vae, timestep_embedder)\n")
        f.write("  3. MoE è·¯ç”±æœºåˆ¶çš„è®¡ç®—å¼€é”€\n")
        f.write("  4. ç”Ÿæˆåˆ†æ”¯ä¸“å®¶çš„å‚æ•°å’Œè®¡ç®—\n")

        f.write("="*80 + "\n")

    print(f"   âœ… è¯¦ç»†ç»Ÿè®¡å·²ä¿å­˜: {OUTPUT_SUMMARY}")

def main():
    """Main function."""
    print("\n" + "="*80)
    print("ğŸ“Š Benchmark Results Comparison")
    print("="*80 + "\n")

    # Check files
    check_files_exist()

    # Load data
    df_baseline, df_mot = load_data()

    # Filter warmup period
    print(f"\nğŸ” è¿‡æ»¤ warmup é˜¶æ®µ (å‰ {WARMUP_STEPS} æ­¥)...")
    df_baseline_stable = df_baseline[df_baseline['step'] >= WARMUP_STEPS].copy()
    df_mot_stable = df_mot[df_mot['step'] >= WARMUP_STEPS].copy()
    print(f"   Baseline: {len(df_baseline_stable)} æ­¥ç”¨äºç»Ÿè®¡")
    print(f"   MoT:      {len(df_mot_stable)} æ­¥ç”¨äºç»Ÿè®¡")

    # Compute statistics
    print("\nğŸ“ˆ è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡...")
    baseline_stats = compute_stats(df_baseline_stable, "Baseline")
    mot_stats = compute_stats(df_mot_stable, "MoT")

    # Print comparison
    print_comparison_table(baseline_stats, mot_stats)

    # Plot comparison
    plot_comparison(df_baseline, df_mot, baseline_stats, mot_stats)

    # Save summary
    save_summary(baseline_stats, mot_stats)

    print("\nâœ… åˆ†æå®Œæˆï¼")
    print(f"   ğŸ“Š å¯è§†åŒ–å›¾è¡¨: {OUTPUT_PLOT}")
    print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š:   {OUTPUT_SUMMARY}")
    print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    main()
