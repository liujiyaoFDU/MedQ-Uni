# SSIM Loss v3: 推理崩溃排查与模型加载修复

> 日期: 2026-02-06
> 前序版本: v2 (见 `20260206_SSIM_Loss_v2_梯度缩放修复方案.md`)
> 状态: 已修复, 待训练验证

---

## 1. 问题: v2 推理崩溃

### 1.1 现象

v2 checkpoint (`stage1_medq_2nodes_unif_eyeQ1_sr_ssim_loss_v2/0004000`) 推理时:

- **文本生成崩溃**: `TypeError: sequence item 600: expected str instance, NoneType found`
- **图像生成输出一团糟**: 完全不可用
- **对比**: Pixel loss 训练的 checkpoint 推理正常

### 1.2 错误堆栈

```
File "inference_pipeline/MedQ-Uni_run_batch_test2.py", line 530
  → process_single_sample → infer_understanding_text
File "inferencer.py", line 306
  → output = self.tokenizer.decode(unpacked_latent[:,0])
File "modeling/qwen2/tokenization_qwen2.py", line 274
  → text = "".join(tokens)
TypeError: sequence item 600: expected str instance, NoneType found
```

**直接原因**: 模型生成的 token ID 在 `self.decoder` 字典中不存在,
`_convert_id_to_token()` 返回 `None`, `"".join(tokens)` 无法拼接 `None`.

### 1.3 加载时的警告信号

```
The safetensors archive does not contain metadata.
Make sure to save your model with the `save_pretrained` method.

Some weights were not used when initializing Bagel:
  vit_model.vision_model.encoder.layers.24.self_attn.q_proj.weight
  vit_model.vision_model.encoder.layers.24.self_attn.k_proj.bias
  ... (共 16 个 layer 24 权重)
```

---

## 2. 排查过程

### 2.1 初步假设: SSIM 梯度腐蚀 LLM 权重

**假设**: SSIM loss 的有理函数梯度通过 VAE decoder 反传到 LLM, 在 4000 步内腐蚀了
LLM 权重, 导致模型生成无效 token ID.

**梯度传播路径分析**:

```
last_hidden_state (LLM 输出)
  ↓ llm2vae() 投影
velocity_pred
  ↓ z0 prediction
z0_tokens_hybrid
  ↓ unpatchify
latent_batch
  ↓ vae_decode_fn()  ← VAE 冻结但梯度仍通过
x_pred_chunk
  ↓ [SSIM: _scale_gradient(0.01); Pixel loss: 无缩放!]
loss computation
```

**关键发现**: Pixel loss 和 SSIM loss 的梯度都传到 LLM backbone, 路径完全相同.

**Pixel loss vs SSIM loss 梯度特性对比**:

| | Pixel Loss (L2) | SSIM Loss |
|---|---|---|
| weight | 10000 | 1 |
| grad_scale | 无 | 0.01 |
| max_t | 0.2 | 0.3 |
| 全局梯度量级 | O(100-1000) | O(0.1-1.0) |
| **per-element 梯度** | **平滑** `2(x_pred-x_gt)` | **有极端异常值** (分母≈C2=0.0009) |

SSIM 梯度全局量级反而更小, 但 per-element 有极端异常值 → 梯度方向被少数点主导.

### 2.2 发现脚本参数与注释不一致

**`train_sft_stage1_medq_unif_multinode_eyeQ1_sr_ssim_loss.sh`**:

| 参数 | 注释声称 (v2) | 实际值 | 偏差 |
|------|-------------|--------|------|
| `SSIM_LOSS_WEIGHT` | 0.1 | **1** | 10x 偏大 |
| `SSIM_LOSS_MAX_T` | 0.1 | **0.3** | 3x 偏松 |
| `SSIM_GRAD_SCALE` | 0.1 | 0.01 | 更严格 |

v2 的三层衰减实际上只有 `grad_scale=0.01` 真正在起作用.

### 2.3 真正根因: `finetune_from_ema=False`

**关键发现**: 训练脚本中 `--finetune_from_ema False` 导致模型没有正确加载
EMA 预训练权重, 实际上相当于从头训练.

```bash
# 问题配置:
--finetune_from_ema False   # ← 不加载 EMA 权重, 相当于从头训练!

# 修复后:
--finetune_from_ema True    # ← 正确加载 EMA 预训练权重
```

**推理崩溃的真正原因**:
- 不是 SSIM loss 腐蚀了 LLM 权重
- 而是模型从头训练, 4000 步远远不够, 模型根本没训好
- Pixel loss checkpoint 正常是因为那个训练正确加载了预训练权重

**ViT layer 24 警告的解释**: 也与此相关 — checkpoint 格式或层数配置可能不匹配.

---

## 3. 修复方案

### 3.1 核心修复

**`--finetune_from_ema True`** (用户手动修改, 第 227 行)

### 3.2 保守 SSIM 参数 (防止潜在梯度风险)

虽然根因是模型加载, 但 SSIM 梯度分析在理论上仍然成立. 采用保守参数作为安全措施:

| 参数 | v2 实际 | v3 |
|------|---------|-----|
| `SSIM_LOSS_WEIGHT` | 1 | **0.1** |
| `SSIM_LOSS_MAX_T` | 0.3 | **0.1** |
| `SSIM_GRAD_SCALE` | 0.01 | 0.01 (不变) |
| `EXP_NAME` | `..._v2` | **`..._v3`** |
| `finetune_from_ema` | False | **True** |

### 3.3 实验名

`stage1_medq_2nodes_unif_eyeQ1_sr_ssim_loss_v3`

Checkpoint 目录:
```
/mnt/shared-storage-user/safevl-share/quwanying/MedQbench/MedQ-UNI/
  model_checkpoints/training_stage1/stage1_medq_2nodes_unif_eyeQ1_sr_ssim_loss_v3/
```

---

## 4. 技术备忘: SSIM 梯度风险

以下分析虽不是本次崩溃的直接原因, 但对未来调参有参考价值.

### 4.1 为什么 SSIM per-element 梯度有异常值

SSIM 公式的分母:
```
denominator = σ_x² + σ_y² + C2
其中 C2 = (0.03)² = 0.0009
```

在**均匀/低对比度区域** (医学图像中极常见):
- `σ_x² ≈ 0`, `σ_y² ≈ 0`
- `denominator ≈ 0.0009`
- 梯度 ∝ `1/denominator²` ≈ `1.2×10⁶` per-element

L2 loss 没有除法, 梯度永远是 `2(x_pred - x_gt)`, 量级 O(1).

### 4.2 max_t 的影响

| max_t | latent 噪声比 | σ 估计可靠性 | 梯度异常值频率 |
|-------|-------------|------------|------------|
| 0.1 | 10% | 高 | 低 |
| 0.2 | 20% | 中 | 中 |
| 0.3 | 30% | 低 | 高 |

`max_t=0.1` 时只有 90%+ 信号的样本参与, σ 估计准确, 梯度稳定.

### 4.3 三层衰减的有效量级

v3 配置: `0.1 (weight) × 0.01 (grad_scale) = 0.001`

乘以 SSIM 梯度 O(10-100): 到达 LLM 的有效梯度 ≈ O(0.01-0.1)

与 grad_norm clip=1.0 配合, 完全在安全范围内.

### 4.4 未来调参建议

如果 v3 保守参数效果不够明显 (SSIM 指标提升不大), 可逐步放松:

| 阶段 | weight | max_t | grad_scale | 有效梯度 |
|------|--------|-------|-----------|---------|
| 保守 (v3) | 0.1 | 0.1 | 0.01 | O(0.01-0.1) |
| 中等 | 0.2 | 0.15 | 0.01 | O(0.02-0.2) |
| 较激进 | 0.5 | 0.2 | 0.01 | O(0.05-0.5) |

规则:
- `grad_scale=0.01` 建议不动 (核心安全阀)
- 先调 `weight`, 再调 `max_t`
- 观察 `total_norm` 和推理质量

---

## 5. 版本演进总结

```
v1 (2026-02-05)   SSIM 集成
  ↓ Bug: VAE=None crash → 修复 VAE 集成条件
  ↓ Bug: 推理坍塌 → 诊断为梯度爆炸

v2 (2026-02-06)   三层梯度衰减
  ↓ Bug: 推理崩溃 (TypeError + 图像一团糟)
  ↓ 排查: 发现参数/注释不一致 + 梯度路径分析
  ↓ 真正根因: finetune_from_ema=False (模型未正确加载)

v3 (2026-02-06)   模型加载修复 + 保守参数
  - finetune_from_ema=True (核心修复)
  - weight=0.1, max_t=0.1, grad_scale=0.01 (保守安全配置)
  - 待训练验证
```

---

## 6. 运行方式

```bash
# 执行训练
bash scripts/training/train_sft_stage1_medq_unif_multinode_eyeQ1_sr_ssim_loss.sh

# 带 debug 日志
SSIM_LOSS_DEBUG=1 bash scripts/training/train_sft_stage1_medq_unif_multinode_eyeQ1_sr_ssim_loss.sh
```

---

## 7. 遗留事项

- [ ] v3 训练: 确认模型正确加载 + SSIM loss 正常下降
- [ ] 4000 步推理验证: 文本生成正常 + 图像输出有结构
- [ ] 20000 步长训练稳定性验证
- [ ] 对比 v3 (SSIM, 保守) vs pixel loss 的 PSNR/SSIM/FID
- [ ] 可选: 逐步放松参数 (weight→0.2, max_t→0.15)
- [ ] 可选: per-element gradient clamp 作为额外安全层
- [ ] 考虑: 学习率是否需要针对 SSIM 调整 (当前 2.5e-6)
